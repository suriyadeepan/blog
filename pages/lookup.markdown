---
layout: page
title: Look up Table
subtitle: Machine Learning Cheatsheet
---

## LDA

In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.

## LSA

Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.

## Perplexity

Perplexity is a measure of how well the model has fit the data: a model with lower perplexity assigns higher likelihood to the test responses, so we expect it to be better at predicting responses. Intuitively, a perplexity equal to *k* means that when the model predicts the next word, there are, on average *k* likely candidates. In particular, for the ideal scenario of perplexity equal to 1, we always know exactly what should be the next word. 

* [Smart Reply : Automated Response Suggestion for Email](https://arxiv.org/abs/1606.04870)
